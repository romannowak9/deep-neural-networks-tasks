{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01e4263f",
   "metadata": {},
   "source": [
    "# Lab 1 - Introduction to Deep Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4024334e",
   "metadata": {},
   "source": [
    "Nowadays, it is difficult to find someone who has not encountered neural networks in one form or another. Among the most obvious examples, almost everyone has heard of ChatGPT, which is based on large language models (LLMs ‚Äì Large Language Models), or has used Google Search, which also relies on language models to better understand user queries. However, neural networks are applied in many other fields, such as image recognition, audio analysis, computer games, and even medicine, where they assist in diagnosing diseases. Due to their versatility and rapid development, it is worth learning the basics of how neural networks work and how they can be applied in various domains.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/vision-agh/DNN-Course-media/refs/heads/main/lab1_introduction/figures/google.png\" alt=\"Google AI Response\" width=\"600\"/>\n",
    "\n",
    "\n",
    "The aim of these course is to introduce the fundamental concepts related to neural networks, to learn how to use popular libraries for building and training them, such as PyTorch, and to understand the process of machine learning.\n",
    "\n",
    "During the course, we will explore some of the most popular neural network architectures, such as:\n",
    "- MLP (Multi-Layer Perceptron)\n",
    "- CNN (Convolutional Neural Networks)\n",
    "- RNN (Recurrent Neural Networks)\n",
    "- GNN (Graph Neural Networks)\n",
    "- SNN (Spiking Neural Networks)\n",
    "- Transformers for natural language processing (NLP) as well as computer vision (CV)\n",
    "- Generative models (GANs, Diffusion Models)\n",
    "- and others.\n",
    "\n",
    "We will also cover different machine learning techniques, such as supervised, unsupervised, and reinforcement learning, along with methods like knowledge distillation and transfer learning.\n",
    "\n",
    "As you can see, the scope of material is very broad, and it is impossible to cover everything within a single semester. However, I hope that by the end of the course you will have a solid foundation for further learning and experimenting with neural networks. üôÇ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be6f5b4a",
   "metadata": {},
   "source": [
    "# What is a deep neural network?\n",
    "\n",
    "As we can see in the illustration above, which shows feedback generated by Google Gemini, neural networks are systems inspired by the structure and functioning of the human brain. The division into Artificial Intelligence (AI), Machine Learning (ML), Deep Learning (DL), and Generative AI (GAI) is often confusing, but it can be clarified using the diagram below (source: [link](https://commons.wikimedia.org/wiki/File:Unraveling_AI_Complexity_-_A_Comparative_View_of_AI,_Machine_Learning,_Deep_Learning,_and_Generative_AI.png)):\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/vision-agh/DNN-Course-media/refs/heads/main/lab1_introduction/figures/comparison.png\" alt=\"AI, ML, DL, GAI\" width=\"600\"/>\n",
    "\n",
    "Deep neural networks consist of many interconnected layers. Neurons are organized into layers: the input layer receives the data, hidden layers process it, and the output layer generates the final prediction. Three main elements are essential for training a neural network:\n",
    "\n",
    "1. **Data** ‚Äì Neural networks learn from datasets, which provide information about patterns and dependencies. \n",
    "2. **Architecture** ‚Äì The structure of the network, i.e. the number of layers, the number of neurons in each layer, type of layers and how they are connected. Different architectures are better suited for different types of tasks.\n",
    "3. **Learning algorithm** ‚Äì The method by which the network adjusts its weights and biases based on training data to minimize prediction error. The most commonly used approach is backpropagation, combined with optimizers such as Adam or Stochastic Gradient Descent (SGD).\n",
    "\n",
    "Based on the type of training data, machine learning can be divided into three main categories:\n",
    "- **Supervised Learning** ‚Äì The network learns from labeled data, where each input is paired with a known output (label or value). The goal is to predict correct outputs for unseen data.\n",
    "- **Unsupervised Learning** ‚Äì The network learns from unlabeled data, without known outputs. The goal is to discover hidden patterns or structures, such as clustering or dimensionality reduction.\n",
    "- **Reinforcement Learning** ‚Äì The network learns through interaction with an environment (simulation), receiving rewards or penalties based on its actions. The goal is to learn optimal decision-making strategies within that environment.\n",
    "\n",
    "For now, without going into too much detail, I encourage you to explore the following resources:\n",
    "\n",
    "- https://www.geeksforgeeks.org/machine-learning/supervised-vs-reinforcement-vs-unsupervised/\n",
    "- https://medium.com/@bensalemh300/supervised-vs-unsupervised-vs-reinforcement-learning-a3e7bcf1dd23\n",
    "- https://www.youtube.com/watch?v=aircAruvnKk\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bce876e",
   "metadata": {},
   "source": [
    "### Envirenment setup\n",
    "\n",
    "During the course, we will primarily use Python and the following libraries:\n",
    "\n",
    "* **PyTorch** ([https://pytorch.org/](https://pytorch.org/)) ‚Äì a widely used library for building and training neural networks, popular in both academia and industry.\n",
    "* **PyTorch Geometric** ([https://pytorch-geometric.readthedocs.io/en/latest/](https://pytorch-geometric.readthedocs.io/en/latest/)) ‚Äì an extension of PyTorch designed for working with graphs and graph neural networks.\n",
    "* **snnTorch** ([https://snntorch.readthedocs.io/en/latest/](https://snntorch.readthedocs.io/en/latest/)) ‚Äì a library for building and training spiking neural networks (SNNs).\n",
    "* **PyTorch Lightning** ([https://www.pytorchlightning.ai/](https://www.pytorchlightning.ai/)) ‚Äì a high-level interface for PyTorch that simplifies model training and experiment management. We will start using it later in the course.\n",
    "* **Weights & Biases (W&B)** ([https://wandb.ai/](https://wandb.ai/)) ‚Äì a tool for experiment tracking, result visualization, and machine learning project management. We will use it for logging our experiments, and it integrates seamlessly with PyTorch Lightning.\n",
    "\n",
    "Additionally, we will rely on standard data analysis and visualization libraries, such as **NumPy**, **Pandas**, **Matplotlib**, **OpenCV**, and others.\n",
    "The exercises have been prepared and tested with GPU acceleration using **Jupyter Notebook** and a **Conda virtual environment**. Below is the environment setup guide (for Linux and CUDA 12.8):\n",
    "\n",
    "```bash\n",
    "conda create -n dnn python=3.9\n",
    "conda activate dnn\n",
    "\n",
    "pip3 install torch torchvision --index-url https://download.pytorch.org/whl/cu128\n",
    "pip3 install torch_geometric\n",
    "pip3 install pyg_lib torch_scatter torch_sparse torch_cluster torch_spline_conv -f https://data.pyg.org/whl/torch-2.8.0+cu128.html\n",
    "pip3 install omegaconf opencv-python matplotlib psutil wandb lightning numba pybind11 tqdm pandas tonic\n",
    "pip3 install lightning snntorch\n",
    "```\n",
    "\n",
    "It is possible that during some classes we will need to install additional libraries; I will inform you about this as needed.\n",
    "If you do not have access to a GPU or prefer not to run computations on your own machine, you can use **Google Colab** ([https://colab.research.google.com/](https://colab.research.google.com/)), which provides free (but limited) GPU access in the cloud. In that case, you can copy the Jupyter Notebook code into Colab and run it there. However, keep in mind that library version conflicts may occur, so it is recommended to install the required versions manually in a Colab cell when necessary.\n",
    "\n",
    "\n",
    "## W&B Setup\n",
    "\n",
    "To use Weights & Biases (W&B) for experiment tracking, you need to create an account on their platform. You can sign up for free at [https://wandb.ai/](https://wandb.ai/). After creating an account, you will receive an API key that you will use to log in from your code (User Settings -> Scroll Down and Reveal API keys). \n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/vision-agh/DNN-Course-media/refs/heads/main/lab1_introduction/figures/api.png\" alt=\"W&B API Key\" width=\"600\"/>\n",
    "\n",
    "\n",
    "During the first run of a script that uses W&B, you will be prompted to enter your API key to authenticate your account. You can also log in directly from a Jupyter Notebook cell using the following command:\n",
    "\n",
    "```python\n",
    "!wandb login YOUR_API_KEY\n",
    "```\n",
    "\n",
    "from the terminal:\n",
    "\n",
    "```bash\n",
    "wandb login YOUR_API_KEY\n",
    "```\n",
    "\n",
    "or in Colab:\n",
    "\n",
    "```python\n",
    "!pip install wandb\n",
    "!wandb login YOUR_API_KEY\n",
    "```\n",
    "\n",
    "or set the environment variable `WANDB_API_KEY` with your API key:\n",
    "\n",
    "```python\n",
    "import os\n",
    "import wandb\n",
    "\n",
    "wandb.login()\n",
    "os.environ['WANDB_API_KEY'] = 'YOUR_API_KEY'\n",
    "```\n",
    "\n",
    "All projects and experiments will be organized in W&B dashboard, where you can visualize metrics, compare runs, and manage your machine learning projects. After inviting you to the course project, you will be able to see all the experiments we conduct during the classes under this link: [https://wandb.ai/imperator/deep-neural-network-course](https://wandb.ai/imperator/deep-neural-network-course).\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/vision-agh/DNN-Course-media/refs/heads/main/lab1_introduction/figures/wandb.png\" alt=\"W&B Dashboard\" width=\"600\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dcebc91",
   "metadata": {},
   "source": [
    "# First steps\n",
    "\n",
    "Let's start by importing the necessary libraries and checking if a GPU is available for computations. If a GPU is not available, the code will automatically fall back to using the CPU. If you are using Google Colab, you can enable GPU support by navigating to `Runtime` -> `Change runtime type` and selecting `GPU` from the `Hardware accelerator` dropdown menu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a78cd326",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Check if a GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b7f294d",
   "metadata": {},
   "source": [
    "Now we can proceed to implement our first neural network layer using PyTorch. We will create a simple feedforward neural network with one hidden layer and demonstrate how to perform a forward pass with some sample input data.\n",
    "\n",
    "Documentation:\n",
    "- https://docs.pytorch.org/docs/stable/generated/torch.nn.Linear.html\n",
    "- https://docs.pytorch.org/docs/stable/generated/torch.randn.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "395bbfb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer parameters:\n",
      "weight: torch.Size([5, 10])\n",
      "bias: torch.Size([5])\n",
      "------------------------------\n",
      "Input tensor:\n",
      "Size: torch.Size([1, 10])\n",
      "------------------------------\n",
      "Output tensor:\n",
      "Size: torch.Size([1, 5])\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# Initialize a simple feedforward neural network layer\n",
    "# Here, we create a linear layer with 10 input features and 5 output features\n",
    "# The layer is moved to the appropriate device (GPU or CPU)\n",
    "\n",
    "layer = nn.Linear(in_features=10, out_features=5, bias=True).to(device) \n",
    "\n",
    "# We can visualize the layer's variables (learnable parameters):\n",
    "print(\"Layer parameters:\")\n",
    "for name, param in layer.named_parameters():\n",
    "    print(f\"{name}: {param.size()}\")\n",
    "\n",
    "# We generate a random input tensor with 10 features and 1 batch size (number of samples)\n",
    "# We also move this tensor to the appropriate device (same as the layer)\n",
    "\n",
    "input_tensor = torch.randn(1, 10).to(device)\n",
    "print(\"-\" * 30)\n",
    "print(\"Input tensor:\")\n",
    "print(f\"Size: {input_tensor.size()}\")\n",
    "\n",
    "# Perform a forward pass through the layer\n",
    "output_tensor = layer(input_tensor)\n",
    "print(\"-\" * 30)\n",
    "print(\"Output tensor:\")\n",
    "print(f\"Size: {output_tensor.size()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "11c2adcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input tensor values:\n",
      "tensor([[ 1.8303, -0.8513, -0.7777, -0.7513, -0.0419,  0.4483, -0.7717,  0.1644,\n",
      "          1.2952,  0.3393]], device='cuda:0')\n",
      "Output tensor values:\n",
      "tensor([[-0.8091, -0.2779,  0.0898, -0.9380, -0.0391]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Layer weights:\n",
      "Parameter containing:\n",
      "tensor([[-0.1146,  0.1138, -0.1456, -0.0378,  0.2940, -0.1045,  0.2339,  0.0553,\n",
      "         -0.2838,  0.1705],\n",
      "        [-0.0340, -0.1277,  0.2796,  0.2580, -0.2875, -0.2320, -0.2262,  0.1445,\n",
      "         -0.2920,  0.2664],\n",
      "        [-0.0095, -0.2936, -0.1171, -0.1256, -0.2050,  0.3054, -0.1229,  0.0933,\n",
      "         -0.1647, -0.3030],\n",
      "        [-0.1290,  0.1489,  0.1002, -0.0529, -0.3139, -0.1997,  0.3161, -0.2217,\n",
      "         -0.0023, -0.2787],\n",
      "        [-0.1003, -0.1931,  0.1686, -0.1008,  0.0387,  0.1518, -0.0084, -0.2196,\n",
      "         -0.0247,  0.0846]], device='cuda:0', requires_grad=True)\n",
      "Layer biases:\n",
      "Parameter containing:\n",
      "tensor([-0.1037,  0.2686, -0.2678, -0.0827,  0.0020], device='cuda:0',\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# We can visualize the input and output tensors:\n",
    "print(\"Input tensor values:\")\n",
    "print(input_tensor)\n",
    "print(\"Output tensor values:\")\n",
    "print(output_tensor)\n",
    "\n",
    "# Also print the layer's weights and biases\n",
    "print(\"Layer weights:\")\n",
    "print(layer.weight)\n",
    "print(\"Layer biases:\")\n",
    "print(layer.bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75e738c3",
   "metadata": {},
   "source": [
    "Below is a simple example of how to use the W&B library to log metrics during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e25f1472",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/imperator/Code/DNN-Course/DNN/lab1/wandb/run-20251002_205521-y3ahgy6z</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/deep-neural-network-course/lab1-introduction/runs/y3ahgy6z' target=\"_blank\">Kamil Jeziorek</a></strong> to <a href='https://wandb.ai/deep-neural-network-course/lab1-introduction' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/deep-neural-network-course/lab1-introduction' target=\"_blank\">https://wandb.ai/deep-neural-network-course/lab1-introduction</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/deep-neural-network-course/lab1-introduction/runs/y3ahgy6z' target=\"_blank\">https://wandb.ai/deep-neural-network-course/lab1-introduction/runs/y3ahgy6z</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>‚ñá‚ñÑ‚ñÖ‚ñà‚ñÅ</td></tr><tr><td>epoch</td><td>‚ñÅ‚ñÉ‚ñÖ‚ñÜ‚ñà</td></tr><tr><td>loss</td><td>‚ñÑ‚ñÅ‚ñÇ‚ñÑ‚ñà</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>0.11708</td></tr><tr><td>epoch</td><td>4</td></tr><tr><td>loss</td><td>0.2208</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">Kamil Jeziorek</strong> at: <a href='https://wandb.ai/deep-neural-network-course/lab1-introduction/runs/y3ahgy6z' target=\"_blank\">https://wandb.ai/deep-neural-network-course/lab1-introduction/runs/y3ahgy6z</a><br> View project at: <a href='https://wandb.ai/deep-neural-network-course/lab1-introduction' target=\"_blank\">https://wandb.ai/deep-neural-network-course/lab1-introduction</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20251002_205521-y3ahgy6z/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "# Initialize a W&B run\n",
    "wandb.init(\n",
    "        project=\"lab1-introduction\",            # do not change this line ! \n",
    "                                                # (this is the name of the project for each lab)\n",
    "\n",
    "        entity=\"deep-neural-network-course\",    # do not change this line ! \n",
    "                                                # (this is the name of the team for the course)\n",
    "\n",
    "        group=\"test_run\",                       # do not change this line ! \n",
    "                                                # (this is the name of the group for each lab, e.g. task1, task2, etc.)\n",
    "\n",
    "        name=\"Kamil Jeziorek\",                  # change to your name \n",
    "                                                # (e.g. Johny Bravo - your full name)\n",
    "\n",
    "        settings=wandb.Settings(save_code=False)# do not change this line ! \n",
    "                                                # (this is to avoid saving all the code files in W&B, which would be too large)\n",
    ")\n",
    "\n",
    "# Log some example metrics to W&B\n",
    "for epoch in range(5):\n",
    "    # Simulate some metrics\n",
    "    loss = torch.randn(1).item()  # Random loss value\n",
    "    accuracy = torch.rand(1).item()  # Random accuracy value\n",
    "\n",
    "    # Log metrics to W&B\n",
    "    wandb.log({\"epoch\": epoch, \"loss\": loss, \"accuracy\": accuracy})\n",
    "\n",
    "# Finish the W&B run\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ea94048",
   "metadata": {},
   "source": [
    "And you should see the results in your W&B dashboard.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/vision-agh/DNN-Course-media/refs/heads/main/lab1_introduction/figures/test.png\" alt=\"W&B Dashboard Example\" width=\"1000\"/>\n",
    "\n",
    "For now, this is enough to get you started. In the next classes, we will delve deeper into the concepts and techniques of deep learning. If you have any questions or need assistance, feel free to ask!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dnn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
